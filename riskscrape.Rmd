# Scraping EU legislation

This notebook documents the process of scraping two key bits of legislation 
relating to food risks: 
EU regulations 669/2009 and 884/2014. The aim is to extract the list of foods 
from the annex tables and turn it into a useful table.

## Loading the relevant packages.

Scraping is carried out using the `rvest` package, with cleaning functions
from the `tidyverse`.

```{r warning = FALSE,message=FALSE}
library(tidyverse)
library(rvest)
```

## Setting up some functions

### Remove bad rows

Firstly we want to remove any dodgy rows introduced by the markers that 
indicate where the document has been updated, as `rvest` will NOT like these 
and they are not useful to us anyway


```{r }
getbad <- function(x) {
  rows <- html_nodes(x, "tr")
  cells <- lapply(rows, "html_nodes", xpath = ".//td|.//th")
  ncols <- lapply(cells, html_attr, "colspan", default = "1")
  ncols <- lapply(ncols, as.integer)
  bad <- which(is.na(ncols))
  move <- seq(from = 0, by = -1, length.out = length(bad))
  remrows <- bad+move
  return(remrows)
}

removerow <- function(n, table) {
  remove <- table %>% html_nodes("tr") %>% .[n]
  xml_remove(remove)
}

removebad <- function(table) {
  remrows <- getbad(table)
  nobad <- lapply(remrows, removerow, table)
  return(nobad)
}
```


The markers might also appear within the table cells and we want them out of there. 
This function identifies what they are.


```{r }
getedits <- function(x) {
  x %>% 
    html_nodes(xpath = '//*[@class="arrow"]') %>% 
    html_text() %>% 
    trimws() %>% 
    remodd()
}
```


### The main scraping function

Takes in a url, xpath and set of column names, and outputs a tibble


```{r }
scrapetable <- function(url, xpath, colnames) {
  tablescrape <- read_html(url) 
  removebad(tablescrape)
  annex <- tablescrape %>% 
    html_nodes(xpath = xpath) %>% 
    html_table(fill = TRUE) %>% 
    .[[1]] %>% 
    as_tibble() %>% 
    rename_all(., ~colnames)
}
```

### Cleaning

Secondly, some cleaning will be required. 
Some cleaning functions we might want to perform are:

* Splitting lists of TARIC codes into their own columns
* Removing footnotes
* Removing odd characters (usually put there to look nice in the pdf but we don't 
want them, so we remove anything that is not a letter, number, space, 
bracket, comma or dot)
* Removing country codes
* Removing anything that is not a number (for the CN codes)
* Removing the letter+number combination at the beginning of a cell that indicates 
there is an editing indicator in there

```{r }
septaric <- function(x) {
  x %>% 
    separate(CNcode,c(letters[1:6]), sep=";|or|\n") %>%  
    gather(cnn, Code, a:f) %>% 
    filter(!is.na(Code)) %>%
    filter(Code != "") %>% 
    separate(TARICcode,c(letters[1:6]), sep=";|\n") %>%
    gather(tcn, TARIC, a:f) %>% 
    filter(!is.na(TARIC))
}

remfoot <- function(x) {
  x <- gsub("(\\([0-9]+\\))", "", x)
}

remodd <- function(x) {
  x <- gsub("[^A-z0-9 \\(\\),\\.]", "", x)
}

removecc <- function(x) {
  x <- gsub("( \\(.*\\))", "", x)
}

removenn <- function(x) {
  x <- gsub("[^0-9]", "", x)
}

remedits <- function(x) {
  x <- gsub("[A-z][0-9] ", "", x)
}
```

## Regulation 669/2009 - food of non-animal origin

Find the most recent consolidated version at 
https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:32009R0669

The url will look like this, perhaps just a different date on the
end if it has been updated. 

```{r }
url669 <- "https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:02009R0669-20180706"
```

Find the xpath to the relevant table (or make sure it has not changed from the
one below). Also make sure that the table has the six columns contained in the
cols vector, in that order. 

```{r }
xpath669 <- "//*[@id=\"document1\"]/div/div/div[38]/table"
cols669 <- c("Food","CNcode","TARICcode","Country","Hazard","Checks")
```

Then scrape the data, remove any bad rows and turn it into a tibble

```{r }
annex669 <- scrapetable(url669, xpath669, cols669)
```

Now apply cleaning functions to the relevant variables.
The code also separates multiple CN and TARIC codes in a cell and 
creates separate lines for them, as well as getting rid of lines
that are just a category (e.g. "Feed and food")

```{r warning = FALSE,message=FALSE}

edits669 <- getedits(read_html(url669))

annex669clean <- annex669 %>% 
  slice(-1) %>% 
  slice(-n()) %>% 
  septaric(.) %>%   
  select(Food, Code, TARIC, Country, Hazard) %>% 
  mutate_at(vars(Food,Country,Hazard), funs(remfoot(remodd(.)))) %>% 
  mutate_at(vars(Food), funs(gsub("(\\(Food.*)|(\\(Feed.*)", "", .))) %>%
  mutate(Food = remedits(Food)) %>% 
  mutate_at(vars(Country), funs(removecc(.))) %>%
  mutate_at(vars(Code), funs(removenn(.))) %>% 
  mutate_all(., funs(trimws(.))) %>%
  filter(!grepl("^$", Food)) %>%
  filter(!Food %in% edits669) %>% 
  unique()
```

## Regulation 884/2014 - contamination risk by aflatoxins

Find the most recent consolidated version at 
https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:32014R0884


```{r }
url884 <- "https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:02014R0884-20161222"
```

Find the xpath and define the column names - these are different from the above
as there is no 'hazard' column (the hazard is aflatoxins in all cases - this 
column will be added later).

```{r }
xpath884 <- "//*[@id=\"document1\"]/div/div/div[52]/table"
cols884 <- c("Food","CNcode","TARICcode","Country","Checks")
```

Get the table, remove any dodgy rows and turn it into a tibble

```{r warning = FALSE,message=FALSE}
annex884 <- scrapetable(url884, xpath884, cols884)
```

...and perform any relevant cleaning functions (same as above although 
some will not be relevant this time as there is no hazard column).


```{r warning = FALSE,message=FALSE}

edits884 <- getedits(read_html(url884))

annex884clean <- annex884 %>% 
  slice(-1) %>% 
  slice(-n()) %>% 
  septaric(.) %>%   
  select(Food, Code, TARIC, Country) %>% 
  mutate_at(vars(Food,Country), funs(remfoot(remodd(.)))) %>%
  mutate_at(vars(Food), funs(gsub("(\\(Food.*)|(\\(Feed.*)", "", .))) %>% 
  mutate(Food = remedits(Food)) %>% 
  mutate_at(vars(Country), funs(removecc(.))) %>%
  mutate_at(vars(Code), funs(removenn(.))) %>% 
  mutate_all(., funs(trimws(.))) %>%
  filter(!grepl("^$", Food)) %>% 
  filter(!Food %in% edits884) %>% 
  unique() %>% 
  mutate(Hazard = "Aflatoxins")
```

## Bind the tables together

All that's left to do then is bind the two hazard tables together


```{r }
hazards <- bind_rows(annex669clean,annex884clean)
hazards
```

